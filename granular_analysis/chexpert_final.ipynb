{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AhzpgMnrCwNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dce08be-75e0-4b05-a6f3-05e12cb75829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Read files\n",
        "  test_df = pd.read_csv('/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/True_3.csv')\n",
        "  preds_df = pd.read_csv('/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/preds_3.csv')\n",
        "  admission_df = pd.read_csv('/content/drive/MyDrive/UM2ii/CXR Granular Bias/admissions.csv.gz', compression='gzip')\n",
        "  admission_df = admission_df[['subject_id', 'race']]\n",
        "  admission_df = admission_df.rename(columns={\"race\": \"Race\"})\n",
        "  admission_df = admission_df.drop_duplicates()\n",
        "  df = pd.merge(test_df, preds_df)\n",
        "  df['subject_id'] = [i[2][1:] for i in df.path.str.split('/')]\n",
        "  df['subject_id'] = df['subject_id'].astype(int)\n",
        "\n",
        "  # There are individuals who inconsitently report their race (granular and course); exclude\n",
        "  admission_df = admission_df[~admission_df['subject_id'].duplicated(keep=False)]\n",
        "  df = pd.merge(df, admission_df, how = \"left\", on = 'subject_id')\n",
        "  df = df[df.Race != 'OTHER']\n",
        "  df = df[df.Race != 'UNKNOWN']\n",
        "  df = df[df.Race != 'UNABLE TO OBTAIN']\n",
        "  df = df[df.Race != 'MULTIPLE RACE/ETHNICITY']\n",
        "  df = df[df.Race != 'PATIENT DECLINED TO ANSWER']\n",
        "  df = df[df.Race != 'AMERICAN INDIAN/ALASKA NATIVE']\n",
        "  df = df[df.Race != 'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER']\n",
        "\n",
        "print(\"In our test set, we have \" + str(sum(df.Race.value_counts())) + \" individuals with a documented race/ethnicity.\" )\n",
        "print(\"The proportion of each individuals in each documented race/ethnicity is as follows:\")\n",
        "print(df.Race.value_counts()/sum(df.Race.value_counts()))\n",
        "print()\n",
        "print(\"The number of each individuals in each documented race/ethnicity is as follows:\")\n",
        "print(df.Race.value_counts())"
      ],
      "metadata": {
        "id": "L3HIk-TkbKEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf7beb0-e960-4d4b-962b-106c0f0ae766"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In our test set, we have 270136 individuals with a documented race/ethnicity.\n",
            "The proportion of each individuals in each documented race/ethnicity is as follows:\n",
            "WHITE                                 0.693954\n",
            "BLACK/AFRICAN AMERICAN                0.163355\n",
            "HISPANIC/LATINO - PUERTO RICAN        0.018517\n",
            "WHITE - OTHER EUROPEAN                0.018372\n",
            "ASIAN - CHINESE                       0.014763\n",
            "WHITE - RUSSIAN                       0.012990\n",
            "BLACK/CAPE VERDEAN                    0.012660\n",
            "HISPANIC/LATINO - DOMINICAN           0.011324\n",
            "ASIAN                                 0.009925\n",
            "BLACK/CARIBBEAN ISLAND                0.006819\n",
            "BLACK/AFRICAN                         0.005023\n",
            "HISPANIC OR LATINO                    0.004612\n",
            "ASIAN - SOUTH EAST ASIAN              0.004128\n",
            "PORTUGUESE                            0.003247\n",
            "ASIAN - ASIAN INDIAN                  0.002891\n",
            "HISPANIC/LATINO - GUATEMALAN          0.002713\n",
            "WHITE - BRAZILIAN                     0.002362\n",
            "WHITE - EASTERN EUROPEAN              0.002299\n",
            "HISPANIC/LATINO - SALVADORAN          0.001836\n",
            "ASIAN - KOREAN                        0.001281\n",
            "HISPANIC/LATINO - HONDURAN            0.001277\n",
            "SOUTH AMERICAN                        0.001203\n",
            "HISPANIC/LATINO - COLUMBIAN           0.001192\n",
            "HISPANIC/LATINO - MEXICAN             0.001114\n",
            "HISPANIC/LATINO - CENTRAL AMERICAN    0.001103\n",
            "HISPANIC/LATINO - CUBAN               0.001040\n",
            "Name: Race, dtype: float64\n",
            "\n",
            "The number of each individuals in each documented race/ethnicity is as follows:\n",
            "WHITE                                 187462\n",
            "BLACK/AFRICAN AMERICAN                 44128\n",
            "HISPANIC/LATINO - PUERTO RICAN          5002\n",
            "WHITE - OTHER EUROPEAN                  4963\n",
            "ASIAN - CHINESE                         3988\n",
            "WHITE - RUSSIAN                         3509\n",
            "BLACK/CAPE VERDEAN                      3420\n",
            "HISPANIC/LATINO - DOMINICAN             3059\n",
            "ASIAN                                   2681\n",
            "BLACK/CARIBBEAN ISLAND                  1842\n",
            "BLACK/AFRICAN                           1357\n",
            "HISPANIC OR LATINO                      1246\n",
            "ASIAN - SOUTH EAST ASIAN                1115\n",
            "PORTUGUESE                               877\n",
            "ASIAN - ASIAN INDIAN                     781\n",
            "HISPANIC/LATINO - GUATEMALAN             733\n",
            "WHITE - BRAZILIAN                        638\n",
            "WHITE - EASTERN EUROPEAN                 621\n",
            "HISPANIC/LATINO - SALVADORAN             496\n",
            "ASIAN - KOREAN                           346\n",
            "HISPANIC/LATINO - HONDURAN               345\n",
            "SOUTH AMERICAN                           325\n",
            "HISPANIC/LATINO - COLUMBIAN              322\n",
            "HISPANIC/LATINO - MEXICAN                301\n",
            "HISPANIC/LATINO - CENTRAL AMERICAN       298\n",
            "HISPANIC/LATINO - CUBAN                  281\n",
            "Name: Race, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The number of 'No Finding' Labels in each documented race/ethnicity is as follows:\")\n",
        "df = df[df.Race.notna()]\n",
        "for race in sorted(df.Race.unique()):\n",
        "  temp_df = df[df.Race==race]\n",
        "  print(str(race) + ' ' + str(sum(temp_df['No Finding']==1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awPn97hdfLq6",
        "outputId": "e53e1bdd-7a78-4a83-8369-576a8de58b2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of 'No Finding' Labels in each documented race/ethnicity is as follows:\n",
            "ASIAN 1017\n",
            "ASIAN - ASIAN INDIAN 323\n",
            "ASIAN - CHINESE 1375\n",
            "ASIAN - KOREAN 84\n",
            "ASIAN - SOUTH EAST ASIAN 385\n",
            "BLACK/AFRICAN 680\n",
            "BLACK/AFRICAN AMERICAN 20375\n",
            "BLACK/CAPE VERDEAN 1537\n",
            "BLACK/CARIBBEAN ISLAND 670\n",
            "HISPANIC OR LATINO 729\n",
            "HISPANIC/LATINO - CENTRAL AMERICAN 131\n",
            "HISPANIC/LATINO - COLUMBIAN 179\n",
            "HISPANIC/LATINO - CUBAN 149\n",
            "HISPANIC/LATINO - DOMINICAN 1344\n",
            "HISPANIC/LATINO - GUATEMALAN 364\n",
            "HISPANIC/LATINO - HONDURAN 134\n",
            "HISPANIC/LATINO - MEXICAN 188\n",
            "HISPANIC/LATINO - PUERTO RICAN 2407\n",
            "HISPANIC/LATINO - SALVADORAN 223\n",
            "PORTUGUESE 217\n",
            "SOUTH AMERICAN 141\n",
            "WHITE 64886\n",
            "WHITE - BRAZILIAN 241\n",
            "WHITE - EASTERN EUROPEAN 239\n",
            "WHITE - OTHER EUROPEAN 1696\n",
            "WHITE - RUSSIAN 1415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "def wauc(true, preds):\n",
        "  test_df = pd.read_csv(true)\n",
        "  preds_df = pd.read_csv(preds)\n",
        "  return metrics.roc_auc_score(y_true = test_df.drop('path', axis=1), y_score = preds_df.drop('path', axis=1),  average='macro', multi_class='ovr')\n",
        "\n",
        "\n",
        "auc_3 = wauc('/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/True_3.csv', '/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/preds_3.csv')\n",
        "auc_6 = wauc('/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/True_6.csv', '/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/preds_6.csv')\n",
        "auc_14 = wauc('/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/True_14.csv', '/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/preds_14.csv')\n",
        "auc_96 = wauc('/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/True_96.csv', '/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/preds_96.csv')\n",
        "auc_99 = wauc('/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/True_99.csv', '/content/drive/MyDrive/UM2ii/CXR Granular Bias/CheXpert/preds_99.csv')\n",
        "\n",
        "print('Average AUC of CheXpert-trained models applied onto MIMIC dataset: ' + str(np.mean([auc_3, auc_6, auc_14, auc_96, auc_99])))\n",
        "confidence_interval = stats.t.interval(0.95, len([auc_3, auc_6, auc_14, auc_96, auc_99])-1, loc=np.mean([auc_3, auc_6, auc_14, auc_96, auc_99]), scale=stats.sem([auc_3, auc_6, auc_14, auc_96, auc_99]))\n",
        "\n",
        "# Print the confidence interval\n",
        "print(\"95% Confidence Interval:\", confidence_interval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq0x_Bew75oU",
        "outputId": "8abe85c9-77e5-4a68-9f82-987bdae86c8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average AUC of CheXpert-trained models applied onto MIMIC dataset: 0.7432132342403976\n",
            "95% Confidence Interval: (0.7378593711757045, 0.7485670973050907)\n"
          ]
        }
      ]
    }
  ]
}